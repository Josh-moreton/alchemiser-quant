#!/usr/bin/env python3
"""Business Unit: data | Status: current.

Comprehensive S3 data lake validation against yfinance and Alpaca.

Validates:
1. Completeness - All symbols from strategy DSL configs exist in S3
2. Freshness - S3 data has the latest complete bar available
3. Integrity - Close prices match reference sources within tolerance
4. Gaps - No missing trading days in the data series

Data Sources:
- yfinance: Adjusted close prices (free, may differ from Alpaca)
- Alpaca: Adjusted close prices with Adjustment.ALL (authoritative source)

Usage:
    make validate-data-lake                    # Validate all configured symbols
    make validate-data-lake symbols=SPY,QQQ   # Validate specific symbols
    make validate-data-lake mark-bad=1        # Mark failed symbols for refetch
    make validate-data-lake debug=1           # Show sample mismatches
"""

from __future__ import annotations

import argparse
import csv
import os
import sys
import warnings
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import boto3
import pandas as pd
import yfinance as yf
from alpaca.data.enums import Adjustment
from alpaca.data.historical import StockHistoricalDataClient
from alpaca.data.requests import StockBarsRequest
from alpaca.data.timeframe import TimeFrame
from botocore.exceptions import ClientError
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Suppress yfinance warnings
warnings.filterwarnings("ignore")

# ============================================================================
# CONFIGURATION
# ============================================================================

# Price tolerance: maximum allowed difference between S3 close and yfinance Adj Close
PRICE_TOLERANCE_PCT = 0.02  # 2%

# S3 bucket for market data (production)
DEFAULT_BUCKET = "alchemiser-prod-market-data"

# Default AWS region
DEFAULT_REGION = "us-east-1"

# Output directory for validation reports
VALIDATION_RESULTS_DIR = Path(__file__).parent.parent.parent / "validation_results"

# ANSI colors for terminal output
COLORS = {
    "green": "\033[92m",
    "yellow": "\033[93m",
    "red": "\033[91m",
    "blue": "\033[94m",
    "bold": "\033[1m",
    "reset": "\033[0m",
}


def colorize(text: str, color: str) -> str:
    """Add ANSI color codes to text."""
    return f"{COLORS.get(color, '')}{text}{COLORS['reset']}"


# ============================================================================
# VALIDATION RESULT DATA CLASSES
# ============================================================================


@dataclass
class SymbolValidationResult:
    """Result of validating a single symbol."""

    symbol: str
    s3_records: int = 0
    yf_records: int = 0
    alpaca_records: int = 0
    s3_last_date: str = ""
    yf_last_date: str = ""
    alpaca_last_date: str = ""
    is_fresh: bool = False
    yf_price_mismatches: int = 0  # mismatches vs yfinance
    alpaca_price_mismatches: int = 0  # mismatches vs Alpaca
    missing_dates: int = 0  # dates in reference but not S3 (gaps)
    errors: list[str] = field(default_factory=list)
    yf_sample_mismatches: list[dict[str, Any]] = field(default_factory=list)
    alpaca_sample_mismatches: list[dict[str, Any]] = field(default_factory=list)
    sample_missing_dates: list[str] = field(default_factory=list)

    @property
    def price_mismatches(self) -> int:
        """Total price mismatches (either source)."""
        return self.yf_price_mismatches + self.alpaca_price_mismatches

    @property
    def is_valid(self) -> bool:
        """Symbol passes all validation checks."""
        return (
            self.is_fresh
            and self.yf_price_mismatches == 0
            and self.alpaca_price_mismatches == 0
            and self.missing_dates == 0
            and not self.errors
        )

    @property
    def status(self) -> str:
        """Human-readable status."""
        if self.errors:
            return "ERROR"
        if not self.is_fresh:
            return "STALE"
        if self.alpaca_price_mismatches > 0:
            return "ALPACA_MISMATCH"
        if self.yf_price_mismatches > 0:
            return "YF_MISMATCH"
        if self.missing_dates > 0:
            return "GAPS"
        return "VALID"


@dataclass
class DataLakeValidationReport:
    """Aggregate report of all symbol validations."""

    bucket: str
    timestamp: datetime
    results: list[SymbolValidationResult] = field(default_factory=list)
    configured_symbols: set[str] = field(default_factory=set)
    s3_symbols: set[str] = field(default_factory=set)

    @property
    def missing_symbols(self) -> set[str]:
        """Symbols configured but not in S3."""
        return self.configured_symbols - self.s3_symbols

    @property
    def valid_count(self) -> int:
        """Count of valid symbols."""
        return sum(1 for r in self.results if r.is_valid)

    @property
    def stale_count(self) -> int:
        """Count of stale symbols (not fresh)."""
        return sum(1 for r in self.results if not r.is_fresh and not r.errors)

    @property
    def mismatch_count(self) -> int:
        """Count of symbols with price mismatches."""
        return sum(1 for r in self.results if r.price_mismatches > 0)

    @property
    def gap_count(self) -> int:
        """Count of symbols with data gaps."""
        return sum(1 for r in self.results if r.missing_dates > 0)

    @property
    def error_count(self) -> int:
        """Count of symbols with errors."""
        return sum(1 for r in self.results if r.errors)


# ============================================================================
# S3 DATA ACCESS
# ============================================================================


def list_s3_symbols(s3_client: Any, bucket: str) -> set[str]:
    """List all symbols in S3 bucket.

    Args:
        s3_client: Boto3 S3 client
        bucket: S3 bucket name

    Returns:
        Set of symbol names (folder prefixes)
    """
    symbols: set[str] = set()
    paginator = s3_client.get_paginator("list_objects_v2")

    for page in paginator.paginate(Bucket=bucket, Delimiter="/"):
        for prefix in page.get("CommonPrefixes", []):
            symbol = prefix["Prefix"].rstrip("/")
            if symbol:
                symbols.add(symbol)

    return symbols


def read_s3_parquet(s3_client: Any, bucket: str, symbol: str) -> pd.DataFrame | None:
    """Read parquet data from S3 for a symbol.

    Args:
        s3_client: Boto3 S3 client
        bucket: S3 bucket name
        symbol: Symbol to read

    Returns:
        DataFrame with market data or None if not found
    """
    try:
        key = f"{symbol}/daily.parquet"
        response = s3_client.get_object(Bucket=bucket, Key=key)

        # Write to temp file and read
        temp_path = Path("/tmp/temp_validation.parquet")
        with temp_path.open("wb") as f:
            f.write(response["Body"].read())

        df = pd.read_parquet(temp_path)
        temp_path.unlink()
        return df

    except ClientError as e:
        if e.response["Error"]["Code"] == "NoSuchKey":
            return None
        raise


# ============================================================================
# YFINANCE DATA ACCESS
# ============================================================================


def fetch_yfinance_data(symbol: str, start: str, end: str) -> pd.DataFrame | None:
    """Fetch yfinance data with Adj Close.

    Args:
        symbol: Ticker symbol
        start: Start date (YYYY-MM-DD)
        end: End date (YYYY-MM-DD), inclusive

    Returns:
        DataFrame with OHLCV + Adj Close or None if fetch failed
    """
    try:
        # Add one day to end because yfinance is exclusive on end date
        end_dt = datetime.strptime(end, "%Y-%m-%d") + timedelta(days=1)

        df = yf.download(
            symbol,
            start=start,
            end=end_dt.strftime("%Y-%m-%d"),
            auto_adjust=False,
            progress=False,
        )

        if df is None or df.empty:
            return None

        # Flatten multi-index columns if needed (yfinance behavior varies)
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)

        return df

    except Exception:
        return None


def get_yfinance_latest_date(symbol: str) -> str | None:
    """Get the latest complete bar date from yfinance.

    Fetches recent data and returns the most recent date available.
    This accounts for weekends, holidays, and market closures.

    Args:
        symbol: Ticker symbol

    Returns:
        Latest date as YYYY-MM-DD string, or None if fetch failed
    """
    try:
        # Fetch last 10 days to ensure we get the latest bar
        end_dt = datetime.now()
        start_dt = end_dt - timedelta(days=10)

        df = yf.download(
            symbol,
            start=start_dt.strftime("%Y-%m-%d"),
            end=end_dt.strftime("%Y-%m-%d"),
            auto_adjust=False,
            progress=False,
        )

        if df is None or df.empty:
            return None

        # Return the last date in the index
        return df.index[-1].strftime("%Y-%m-%d")

    except Exception:
        return None


# ============================================================================
# ALPACA DATA ACCESS
# ============================================================================

# Global Alpaca client (initialized once)
_alpaca_client: StockHistoricalDataClient | None = None


def get_alpaca_client() -> StockHistoricalDataClient | None:
    """Get authenticated Alpaca client.

    Uses environment variables for credentials (from .env or system).
    Supported variable names (tries in order):
    - ALPACA_KEY / ALPACA_SECRET
    - ALPACA_API_KEY / ALPACA_API_SECRET
    - APCA_API_KEY_ID / APCA_API_SECRET_KEY

    Returns:
        StockHistoricalDataClient or None if credentials not found
    """
    global _alpaca_client
    if _alpaca_client is not None:
        return _alpaca_client

    api_key = (
        os.environ.get("ALPACA_KEY")
        or os.environ.get("ALPACA_API_KEY")
        or os.environ.get("APCA_API_KEY_ID")
    )
    api_secret = (
        os.environ.get("ALPACA_SECRET")
        or os.environ.get("ALPACA_API_SECRET")
        or os.environ.get("APCA_API_SECRET_KEY")
    )

    if not api_key or not api_secret:
        return None

    _alpaca_client = StockHistoricalDataClient(api_key, api_secret)
    return _alpaca_client


def fetch_alpaca_data(
    client: StockHistoricalDataClient, symbol: str, start: str, end: str
) -> pd.DataFrame | None:
    """Fetch Alpaca data with Adjustment.ALL.

    Args:
        client: Alpaca StockHistoricalDataClient
        symbol: Ticker symbol
        start: Start date (YYYY-MM-DD)
        end: End date (YYYY-MM-DD), inclusive

    Returns:
        DataFrame with date index and close prices, or None if fetch failed
    """
    try:
        start_dt = datetime.strptime(start, "%Y-%m-%d")
        # Add one day to end to make it inclusive
        end_dt = datetime.strptime(end, "%Y-%m-%d") + timedelta(days=1)

        request = StockBarsRequest(
            symbol_or_symbols=symbol,
            timeframe=TimeFrame.Day,
            start=start_dt,
            end=end_dt,
            adjustment=Adjustment.ALL,
        )

        response = client.get_stock_bars(request)
        if symbol not in response or not response[symbol]:
            return None

        bars = response[symbol]
        data = []
        for bar in bars:
            data.append({
                "date": bar.timestamp.strftime("%Y-%m-%d"),
                "close": float(bar.close),
            })

        df = pd.DataFrame(data)
        df.set_index("date", inplace=True)
        return df

    except Exception:
        return None


def get_alpaca_latest_date(client: StockHistoricalDataClient, symbol: str) -> str | None:
    """Get the latest complete bar date from Alpaca.

    Args:
        client: Alpaca StockHistoricalDataClient
        symbol: Ticker symbol

    Returns:
        Latest date as YYYY-MM-DD string, or None if fetch failed
    """
    try:
        end_dt = datetime.now()
        start_dt = end_dt - timedelta(days=10)

        request = StockBarsRequest(
            symbol_or_symbols=symbol,
            timeframe=TimeFrame.Day,
            start=start_dt,
            end=end_dt,
            adjustment=Adjustment.ALL,
        )

        response = client.get_stock_bars(request)
        if symbol not in response or not response[symbol]:
            return None

        bars = response[symbol]
        if not bars:
            return None

        return bars[-1].timestamp.strftime("%Y-%m-%d")

    except Exception:
        return None


# ============================================================================
# SYMBOL EXTRACTION FROM DSL CONFIGS
# ============================================================================


def get_configured_symbols() -> set[str]:
    """Get all symbols from strategy DSL configurations.

    Uses the symbol_extractor from the data Lambda function.

    Returns:
        Set of unique ticker symbols across all configured strategies
    """
    # Add the layers/shared directory to path for imports
    layers_shared = Path(__file__).parent.parent.parent / "layers" / "shared"
    functions_data = Path(__file__).parent.parent.parent / "functions" / "data"

    if str(layers_shared) not in sys.path:
        sys.path.insert(0, str(layers_shared))
    if str(functions_data) not in sys.path:
        sys.path.insert(0, str(functions_data))

    try:
        from symbol_extractor import get_all_configured_symbols

        return get_all_configured_symbols()
    except ImportError as e:
        print(f"Warning: Could not import symbol_extractor: {e}")
        print("Falling back to S3 symbol list only")
        return set()


# ============================================================================
# VALIDATION LOGIC
# ============================================================================


def validate_symbol(
    symbol: str,
    s3_client: Any,
    bucket: str,
    alpaca_client: StockHistoricalDataClient | None = None,
    debug: bool = False,
) -> SymbolValidationResult:
    """Validate a single symbol against yfinance and Alpaca.

    Checks:
    1. Freshness - S3 has the latest bar available
    2. Price integrity - Close prices match reference sources within tolerance
    3. Gaps - No missing trading days

    Args:
        symbol: Symbol to validate
        s3_client: Boto3 S3 client
        bucket: S3 bucket name
        alpaca_client: Optional Alpaca client for Alpaca validation
        debug: Show detailed debug output

    Returns:
        SymbolValidationResult with validation details
    """
    result = SymbolValidationResult(symbol=symbol)

    # 1. Read S3 data
    s3_df = read_s3_parquet(s3_client, bucket, symbol)
    if s3_df is None:
        result.errors.append("No S3 data")
        return result

    result.s3_records = len(s3_df)

    # Normalize S3 timestamps to date strings
    s3_df["date"] = pd.to_datetime(s3_df["timestamp"]).dt.strftime("%Y-%m-%d")
    s3_by_date = s3_df.set_index("date")["close"].to_dict()

    s3_start = min(s3_by_date.keys())
    s3_end = max(s3_by_date.keys())
    result.s3_last_date = s3_end

    # 2. Get yfinance latest date for freshness check
    yf_latest = get_yfinance_latest_date(symbol)
    if yf_latest is None:
        result.errors.append("Failed to fetch yfinance latest date")
        return result

    result.yf_last_date = yf_latest

    # 3. Freshness check - S3 must have the same latest date as yfinance
    result.is_fresh = s3_end >= yf_latest

    # 4. Fetch full yfinance data for price comparison
    yf_df = fetch_yfinance_data(symbol, s3_start, yf_latest)
    if yf_df is None:
        result.errors.append("Failed to fetch yfinance data")
        return result

    # Flatten multi-index if needed
    if isinstance(yf_df.columns, pd.MultiIndex):
        yf_df.columns = yf_df.columns.get_level_values(0)

    result.yf_records = len(yf_df)

    # Use Adj Close for comparison (accounts for splits/dividends)
    adj_close_col = "Adj Close" if "Adj Close" in yf_df.columns else "Close"
    yf_df["date"] = yf_df.index.strftime("%Y-%m-%d")
    yf_by_date = yf_df.set_index("date")[adj_close_col].to_dict()

    # 5. Compare S3 vs yfinance prices and check for gaps
    for date, yf_price in yf_by_date.items():
        if date not in s3_by_date:
            # Gap - date in yfinance but not in S3
            result.missing_dates += 1
            if len(result.sample_missing_dates) < 5:
                result.sample_missing_dates.append(date)
            continue

        s3_price = s3_by_date[date]

        # Avoid division by zero
        if max(abs(s3_price), abs(yf_price)) == 0:
            continue

        pct_diff = abs(s3_price - yf_price) / max(abs(s3_price), abs(yf_price))

        if pct_diff > PRICE_TOLERANCE_PCT:
            result.yf_price_mismatches += 1
            if len(result.yf_sample_mismatches) < 5:
                result.yf_sample_mismatches.append({
                    "date": date,
                    "s3_close": round(s3_price, 4),
                    "ref_close": round(yf_price, 4),
                    "diff_pct": round(pct_diff * 100, 2),
                })

    # 6. Alpaca validation (if client available)
    if alpaca_client is not None:
        # Get Alpaca latest date
        alpaca_latest = get_alpaca_latest_date(alpaca_client, symbol)
        if alpaca_latest:
            result.alpaca_last_date = alpaca_latest

            # Fetch Alpaca data for comparison
            alpaca_df = fetch_alpaca_data(alpaca_client, symbol, s3_start, alpaca_latest)
            if alpaca_df is not None:
                result.alpaca_records = len(alpaca_df)
                alpaca_by_date = alpaca_df["close"].to_dict()

                # Compare S3 vs Alpaca prices
                for date, alpaca_price in alpaca_by_date.items():
                    if date not in s3_by_date:
                        # Already counted as gap from yfinance check
                        continue

                    s3_price = s3_by_date[date]

                    # Avoid division by zero
                    if max(abs(s3_price), abs(alpaca_price)) == 0:
                        continue

                    pct_diff = abs(s3_price - alpaca_price) / max(
                        abs(s3_price), abs(alpaca_price)
                    )

                    if pct_diff > PRICE_TOLERANCE_PCT:
                        result.alpaca_price_mismatches += 1
                        if len(result.alpaca_sample_mismatches) < 5:
                            result.alpaca_sample_mismatches.append({
                                "date": date,
                                "s3_close": round(s3_price, 4),
                                "ref_close": round(alpaca_price, 4),
                                "diff_pct": round(pct_diff * 100, 2),
                            })
            else:
                result.errors.append("Failed to fetch Alpaca data")
        else:
            result.errors.append("Failed to fetch Alpaca latest date")

    return result


# ============================================================================
# BAD DATA MARKERS (for automatic refetch)
# ============================================================================


def write_bad_data_markers(results: list[SymbolValidationResult], region: str) -> int:
    """Write DynamoDB markers for symbols that need refetch.

    Args:
        results: List of validation results
        region: AWS region

    Returns:
        Number of markers written
    """
    # Import the marker service
    layers_shared = Path(__file__).parent.parent.parent / "layers" / "shared"
    if str(layers_shared) not in sys.path:
        sys.path.insert(0, str(layers_shared))

    try:
        from the_alchemiser.data_v2.bad_data_marker_service import BadDataMarkerService
    except ImportError:
        print("Warning: Could not import BadDataMarkerService")
        return 0

    table_name = "alchemiser-bad-data-markers"
    marker_service = BadDataMarkerService(table_name=table_name)
    count = 0

    for r in results:
        if r.is_valid:
            continue

        # Determine reason
        if not r.is_fresh:
            reason = "stale_data"
        elif r.price_mismatches > 0:
            reason = "price_mismatch"
        elif r.missing_dates > 0:
            reason = "data_gaps"
        else:
            reason = "validation_error"

        if marker_service.mark_symbol_for_refetch(
            symbol=r.symbol,
            reason=reason,
            start_date=r.s3_last_date,
            end_date=r.yf_last_date,
            detected_ratio=None,
            source="validate_data_lake",
        ):
            count += 1
            print(f"  Marked {r.symbol} for refetch ({reason})")

    return count


# ============================================================================
# REPORTING
# ============================================================================


def write_csv_report(report: DataLakeValidationReport, output_path: Path) -> None:
    """Write validation report to CSV.

    Args:
        report: Validation report
        output_path: Path to output CSV file
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            "Symbol",
            "Status",
            "S3 Records",
            "YF Records",
            "Alpaca Records",
            "S3 Last Date",
            "YF Last Date",
            "Alpaca Last Date",
            "Is Fresh",
            "YF Mismatches",
            "Alpaca Mismatches",
            "Missing Dates",
            "Errors",
        ])

        for r in report.results:
            writer.writerow([
                r.symbol,
                r.status,
                r.s3_records,
                r.yf_records,
                r.alpaca_records,
                r.s3_last_date,
                r.yf_last_date,
                r.alpaca_last_date,
                r.is_fresh,
                r.yf_price_mismatches,
                r.alpaca_price_mismatches,
                r.missing_dates,
                "; ".join(r.errors) if r.errors else "",
            ])


def print_report(
    report: DataLakeValidationReport, debug: bool = False, alpaca_enabled: bool = True
) -> None:
    """Print formatted validation report to terminal.

    Args:
        report: Validation report
        debug: Show detailed debug output
        alpaca_enabled: Whether Alpaca validation was enabled
    """
    print()
    print(colorize("=" * 70, "bold"))
    print(colorize(" Data Lake Validation Report", "bold"))
    print(colorize("=" * 70, "bold"))
    print()
    print(f"Bucket: {report.bucket}")
    print(f"Timestamp: {report.timestamp.isoformat()}")
    print(f"Configured Symbols: {len(report.configured_symbols)}")
    print(f"S3 Symbols: {len(report.s3_symbols)}")
    print(f"Data Sources: yfinance" + (" + Alpaca" if alpaca_enabled else ""))
    print()

    # Missing symbols check
    if report.missing_symbols:
        print(colorize("‚ö†Ô∏è  Missing from S3:", "yellow"))
        for s in sorted(report.missing_symbols):
            print(f"   {s}")
        print()

    # Summary counts
    yf_mismatch_count = sum(1 for r in report.results if r.yf_price_mismatches > 0)
    alpaca_mismatch_count = sum(
        1 for r in report.results if r.alpaca_price_mismatches > 0
    )

    print(colorize("üìä Validation Summary:", "bold"))
    print(f"   {colorize('‚úì Valid:', 'green')} {report.valid_count}")
    print(f"   {colorize('‚è∞ Stale:', 'yellow')} {report.stale_count}")
    print(f"   {colorize('‚ö° YF Mismatches:', 'red')} {yf_mismatch_count}")
    if alpaca_enabled:
        print(f"   {colorize('‚ö° Alpaca Mismatches:', 'red')} {alpaca_mismatch_count}")
    print(f"   {colorize('üìÖ Data Gaps:', 'yellow')} {report.gap_count}")
    print(f"   {colorize('‚ùå Errors:', 'red')} {report.error_count}")
    print()

    # Detailed results for invalid symbols
    invalid_results = [r for r in report.results if not r.is_valid]
    if invalid_results:
        print(colorize("üîç Invalid Symbols:", "bold"))
        for r in sorted(invalid_results, key=lambda x: x.symbol):
            status_color = "red" if r.errors or r.price_mismatches > 0 else "yellow"
            print(f"   {colorize(r.symbol, status_color)}: {r.status}")
            print(f"      S3 last: {r.s3_last_date} | YF last: {r.yf_last_date}")
            if alpaca_enabled and r.alpaca_last_date:
                print(f"      Alpaca last: {r.alpaca_last_date}")

            if r.yf_price_mismatches > 0:
                print(f"      YF mismatches: {r.yf_price_mismatches}")
                if debug and r.yf_sample_mismatches:
                    for m in r.yf_sample_mismatches[:3]:
                        print(
                            f"        {m['date']}: S3={m['s3_close']}, "
                            f"YF={m['ref_close']}, diff={m['diff_pct']}%"
                        )

            if r.alpaca_price_mismatches > 0:
                print(f"      Alpaca mismatches: {r.alpaca_price_mismatches}")
                if debug and r.alpaca_sample_mismatches:
                    for m in r.alpaca_sample_mismatches[:3]:
                        print(
                            f"        {m['date']}: S3={m['s3_close']}, "
                            f"Alpaca={m['ref_close']}, diff={m['diff_pct']}%"
                        )

            if r.missing_dates > 0:
                print(f"      Missing dates: {r.missing_dates}")
                if debug and r.sample_missing_dates:
                    print(f"        Sample: {', '.join(r.sample_missing_dates[:5])}")

            if r.errors:
                print(f"      Errors: {', '.join(r.errors)}")
        print()

    # Overall status
    print(colorize("=" * 70, "bold"))
    if report.valid_count == len(report.results) and not report.missing_symbols:
        print(colorize("‚úÖ OVERALL: Data lake is healthy!", "green"))
    else:
        issues = len(report.results) - report.valid_count + len(report.missing_symbols)
        print(colorize(f"‚ö†Ô∏è  OVERALL: {issues} symbols need attention", "yellow"))
    print(colorize("=" * 70, "bold"))
    print()


# ============================================================================
# MAIN
# ============================================================================


def main() -> int:
    """Main entry point.

    Returns:
        Exit code (0 = success, 1 = validation failures)
    """
    parser = argparse.ArgumentParser(
        description="Validate S3 data lake against yfinance and Alpaca",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--symbols",
        type=str,
        help="Comma-separated list of symbols to validate (default: all configured)",
    )
    parser.add_argument(
        "--bucket",
        type=str,
        default=DEFAULT_BUCKET,
        help=f"S3 bucket name (default: {DEFAULT_BUCKET})",
    )
    parser.add_argument(
        "--region",
        type=str,
        default=DEFAULT_REGION,
        help=f"AWS region (default: {DEFAULT_REGION})",
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Output CSV path (default: validation_results/data_lake_validation_<date>.csv)",
    )
    parser.add_argument(
        "--mark-bad",
        action="store_true",
        help="Write DynamoDB markers for symbols that need refetch",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Show detailed debug output (sample mismatches, etc.)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="Limit number of symbols to validate (for testing)",
    )

    args = parser.parse_args()

    # Initialize S3 client
    s3_client = boto3.client("s3", region_name=args.region)

    # Initialize Alpaca client (may be None if credentials not available)
    alpaca_client = get_alpaca_client()
    if alpaca_client:
        print(colorize("‚úì Alpaca client initialized", "green"))
    else:
        print(colorize("‚ö†Ô∏è  Alpaca credentials not found - skipping Alpaca validation", "yellow"))
        print("   Set ALPACA_KEY and ALPACA_SECRET in .env file to enable")
    print()

    # Get symbols to validate
    if args.symbols:
        symbols_to_validate = set(s.strip().upper() for s in args.symbols.split(","))
        configured_symbols = symbols_to_validate
    else:
        configured_symbols = get_configured_symbols()
        symbols_to_validate = configured_symbols

    # Get S3 symbols
    print("Scanning S3 bucket for symbols...")
    s3_symbols = list_s3_symbols(s3_client, args.bucket)
    print(f"Found {len(s3_symbols)} symbols in S3")

    # Filter to only symbols that exist in S3
    symbols_to_validate = symbols_to_validate & s3_symbols

    if args.limit:
        symbols_to_validate = set(sorted(symbols_to_validate)[:args.limit])

    print(f"Validating {len(symbols_to_validate)} symbols...")
    print()

    # Create report
    report = DataLakeValidationReport(
        bucket=args.bucket,
        timestamp=datetime.now(),
        configured_symbols=configured_symbols,
        s3_symbols=s3_symbols,
    )

    # Validate each symbol
    for i, symbol in enumerate(sorted(symbols_to_validate), 1):
        status_prefix = f"[{i}/{len(symbols_to_validate)}]"
        print(f"{status_prefix} Validating {symbol}...", end=" ", flush=True)

        result = validate_symbol(
            symbol, s3_client, args.bucket, alpaca_client=alpaca_client, debug=args.debug
        )
        report.results.append(result)

        # Print inline status
        if result.is_valid:
            print(colorize("‚úì", "green"))
        else:
            print(colorize(f"‚úó ({result.status})", "red"))

    # Write CSV report
    if args.output:
        output_path = Path(args.output)
    else:
        date_str = datetime.now().strftime("%Y-%m-%d")
        output_path = VALIDATION_RESULTS_DIR / f"data_lake_validation_{date_str}.csv"

    write_csv_report(report, output_path)
    print(f"\nReport saved to: {output_path}")

    # Print summary
    print_report(report, debug=args.debug, alpaca_enabled=alpaca_client is not None)

    # Mark bad symbols if requested
    if args.mark_bad:
        invalid_results = [r for r in report.results if not r.is_valid]
        if invalid_results:
            print("Writing DynamoDB markers for refetch...")
            markers = write_bad_data_markers(invalid_results, args.region)
            print(f"Marked {markers} symbols for refetch")

    # Return exit code
    all_valid = report.valid_count == len(report.results) and not report.missing_symbols
    return 0 if all_valid else 1


if __name__ == "__main__":
    sys.exit(main())
