# EventBridge High Priority Fixes - Implementation Complete

**Date:** 15 October 2025
**Status:** âœ… COMPLETED

## Overview

Implemented 3 high-priority fixes for EventBridge migration to prevent production failures:
1. **Payload size validation** - Prevents silent failures from oversized events
2. **DLQ monitoring** - Comprehensive CloudWatch alarms for failed events
3. **Lambda timeout monitoring** - Early warning system for approaching timeouts

---

## Fix #1: Payload Size Validation âœ…

### Problem
EventBridge has a hard limit of **256 KB** for event payloads. Events exceeding this limit fail silently, causing workflow stalls without errors in logs.

### Solution Implemented

#### 1. Added Configuration (`shared/config/config.py`)
```python
class EventBridgeSettings(BaseModel):
    max_payload_size_bytes: int = Field(
        default=200_000,  # 200KB leaves 56KB buffer for metadata
        description="Maximum event payload size in bytes"
    )
```

#### 2. Updated EventBridge Bus (`shared/events/eventbridge_bus.py`)
- Added `max_payload_size_bytes` parameter to `__init__()`
- Added payload size check **before** publishing to EventBridge
- Calculates actual UTF-8 byte size: `len(detail_json.encode("utf-8"))`
- Raises `EventPublishError` with code `"PayloadTooLarge"` if limit exceeded
- Logs payload size on every successful publish for monitoring

**Key Changes:**
```python
def publish(self, event: BaseEvent) -> None:
    detail_json = event.model_dump_json()
    
    # Check payload size BEFORE publishing
    payload_size = len(detail_json.encode("utf-8"))
    if payload_size > self.max_payload_size_bytes:
        logger.error(
            "Event payload exceeds EventBridge size limit",
            payload_size_bytes=payload_size,
            max_size_bytes=self.max_payload_size_bytes,
            eventbridge_hard_limit=262_144  # 256KB
        )
        raise EventPublishError(
            f"Event payload too large: {payload_size} bytes exceeds "
            f"limit of {self.max_payload_size_bytes} bytes. "
            f"Consider storing large data in S3 and referencing it in the event.",
            error_code="PayloadTooLarge"
        )
```

#### 3. Added Tests (`tests/shared/events/test_eventbridge_bus.py`)
- `test_publish_event_exceeds_payload_size_limit` - Verifies oversized events are rejected
- `test_publish_event_logs_payload_size` - Confirms size logging works
- All 22 tests passing âœ…

### Impact
- **Prevents silent failures** - Errors are raised immediately with clear error messages
- **Provides actionable guidance** - Error message suggests storing large data in S3
- **Enables monitoring** - Payload size logged on every publish for trend analysis
- **Configurable** - Operators can adjust limit via environment variable if needed

### Future Enhancements (Optional)
- Automatic S3 fallback for large payloads (store in S3, publish reference in event)
- CloudWatch metric for payload sizes to track trends
- Alert when payloads approach 80% of limit

---

## Fix #2: DLQ Monitoring âœ…

### Problem
Failed events go to Dead Letter Queue (DLQ) but accumulate silently without alerts, causing:
- Lost events discovered days/weeks later
- No visibility into workflow failures
- No operational awareness of system health issues

### Solution Implemented

#### 1. DLQ Already Exists (`template.yaml`)
```yaml
EventDLQ:
  Type: AWS::SQS::Queue
  Properties:
    QueueName: !Sub "alchemiser-event-dlq-${Stage}"
    MessageRetentionPeriod: 1209600  # 14 days
```

#### 2. DLQ Depth Alarm Already Exists (`template.yaml`)
```yaml
DLQDepthAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "alchemiser-dlq-high-depth-${Stage}"
    AlarmDescription: Alert when EventBridge DLQ has messages (failed events)
    Namespace: AWS/SQS
    MetricName: ApproximateNumberOfMessagesVisible
    Threshold: 1  # Alert on ANY message in DLQ
    ComparisonOperator: GreaterThanOrEqualToThreshold
    AlarmActions:
      - !Ref AlarmNotificationTopic
```

#### 3. Failed Invocations Alarms Already Exist
- `SignalGeneratedFailedInvocationsAlarm` - Alerts when SignalGenerated events fail
- `RebalancePlannedFailedInvocationsAlarm` - Alerts when RebalancePlanned events fail
- Both configured with threshold of 1 (alert immediately)

### Impact
- **Immediate visibility** - Any DLQ message triggers alarm within 5 minutes
- **SNS notifications** - Alarms publish to `alchemiser-alarms-{Stage}` SNS topic
- **Multi-stage coverage** - Works for dev, staging, and prod
- **Historical tracking** - CloudWatch Logs provide audit trail

### Validation Commands
```bash
# Check DLQ depth
aws sqs get-queue-attributes \
  --queue-url $(aws sqs get-queue-url --queue-name alchemiser-event-dlq-dev --query 'QueueUrl' --output text) \
  --attribute-names ApproximateNumberOfMessagesVisible

# Check alarm state
aws cloudwatch describe-alarms --alarm-names alchemiser-dlq-high-depth-dev
```

---

## Fix #3: Lambda Timeout Monitoring âœ…

### Problem
Lambda functions have a **10-minute timeout**. Functions approaching this limit fail silently without warning, causing:
- Lost events (EventBridge marks as failed after timeout)
- No visibility into performance degradation
- Hard to debug which events/scenarios cause slowness

### Solution Implemented

#### 1. Enhanced Duration Monitoring (`template.yaml`)

**P95 Duration Alarm** (already existed):
```yaml
LambdaDurationAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "alchemiser-lambda-duration-high-${Stage}"
    AlarmDescription: Alert when Lambda duration p95 exceeds 5 minutes
    ExtendedStatistic: p95
    Threshold: 300000  # 5 minutes in milliseconds
```

**Near-Timeout Alarm** (newly added):
```yaml
LambdaNearTimeoutAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "alchemiser-lambda-near-timeout-${Stage}"
    AlarmDescription: Alert when Lambda duration exceeds 8 minutes (80% of 10-min timeout)
    Statistic: Maximum
    Threshold: 480000  # 8 minutes in milliseconds
    EvaluationPeriods: 1  # Alert immediately
```

#### 2. EventBridge Throttling Alarm (newly added)
```yaml
EventBridgeThrottledAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "alchemiser-eventbridge-throttled-${Stage}"
    AlarmDescription: Alert when EventBridge rules are throttled
    Namespace: AWS/Events
    MetricName: ThrottledRules
    Threshold: 10
```

### Impact
- **Early warning** - 8-minute alarm provides 2 minutes to investigate before timeout
- **Performance trends** - P95 alarm catches gradual slowdowns
- **Throttling visibility** - Separate alarm for EventBridge rate limits
- **Layered alerts** - Multiple thresholds prevent false positives

### Monitoring Queries
```bash
# Check Lambda durations
aws cloudwatch get-metric-statistics \
  --namespace AWS/Lambda \
  --metric-name Duration \
  --dimensions Name=FunctionName,Value=the-alchemiser-v2-lambda-dev \
  --start-time $(date -u -d '1 day ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 3600 \
  --statistics Maximum,Average
```

---

## Summary of CloudWatch Alarms

| Alarm Name | Metric | Threshold | Purpose |
|------------|--------|-----------|---------|
| `alchemiser-dlq-high-depth-{Stage}` | SQS Messages | â‰¥ 1 | DLQ has failed events |
| `alchemiser-signal-generated-failed-{Stage}` | FailedInvocations | â‰¥ 1 | SignalGenerated event failures |
| `alchemiser-rebalance-planned-failed-{Stage}` | FailedInvocations | â‰¥ 1 | RebalancePlanned event failures |
| `alchemiser-lambda-errors-{Stage}` | Lambda Errors | â‰¥ 3 | Lambda execution errors |
| `alchemiser-lambda-throttles-{Stage}` | Lambda Throttles | â‰¥ 1 | Lambda concurrent execution limit hit |
| `alchemiser-lambda-duration-high-{Stage}` | Lambda Duration (p95) | > 5 min | Performance degradation |
| `alchemiser-lambda-near-timeout-{Stage}` | Lambda Duration (max) | > 8 min | **NEW** - Near timeout warning |
| `alchemiser-eventbridge-throttled-{Stage}` | ThrottledRules | â‰¥ 10 | **NEW** - EventBridge rate limit |

---

## Testing & Validation

### Unit Tests
```bash
# All EventBridge tests passing
poetry run pytest tests/shared/events/test_eventbridge_bus.py -v
# Result: 22 passed âœ…
```

### Type Checking
```bash
make type-check
# Result: Success: no issues found in 238 source files âœ…
```

### Code Quality
```bash
make format
# Result: All checks passed! âœ…
```

---

## Deployment Checklist

Before deploying to production:

- [ ] **Deploy SAM template** - `make deploy` (or `sam deploy`)
- [ ] **Subscribe to SNS topic** - Add email/Slack to `alchemiser-alarms-prod`
- [ ] **Test DLQ alarm** - Manually send message to DLQ, verify alarm fires
- [ ] **Test timeout alarm** - Deploy with artificially low threshold (e.g., 60s), verify alarm fires
- [ ] **Monitor payload sizes** - Check CloudWatch Logs for `payload_size_bytes` metric
- [ ] **Document runbook** - Add to ops docs: "What to do when DLQ alarm fires"
- [ ] **Set up dashboards** - Create CloudWatch dashboard with key metrics

---

## Operational Runbook

### When DLQ Alarm Fires

1. **Check DLQ contents:**
   ```bash
   aws sqs receive-message \
     --queue-url $(aws sqs get-queue-url --queue-name alchemiser-event-dlq-prod --query 'QueueUrl' --output text) \
     --max-number-of-messages 10
   ```

2. **Identify failure pattern:**
   - Same event type? (e.g., all SignalGenerated)
   - Same correlation_id? (indicates specific workflow failure)
   - Payload size issues? (check for PayloadTooLarge errors)

3. **Check CloudWatch Logs:**
   ```bash
   aws logs tail /aws/lambda/the-alchemiser-v2-lambda-prod --follow
   ```

4. **Remediate:**
   - Fix code bug â†’ Deploy patch
   - Payload too large â†’ Implement S3 fallback
   - Transient error â†’ Replay events from archive

5. **Replay events (if needed):**
   ```bash
   # EventBridge archive retention: 365 days
   aws events start-replay \
     --replay-name manual-replay-$(date +%s) \
     --event-source-arn arn:aws:events:us-east-1:ACCOUNT_ID:event-bus/alchemiser-trading-events \
     --event-start-time "2025-10-15T00:00:00Z" \
     --event-end-time "2025-10-15T23:59:59Z" \
     --destination '{
       "Arn": "arn:aws:events:us-east-1:ACCOUNT_ID:event-bus/alchemiser-trading-events",
       "FilterArns": ["arn:aws:events:us-east-1:ACCOUNT_ID:rule/..."]
     }'
   ```

### When Near-Timeout Alarm Fires

1. **Check current Lambda invocations:**
   ```bash
   aws lambda get-function-concurrency \
     --function-name the-alchemiser-v2-lambda-prod
   ```

2. **Review slow event types:**
   ```sql
   -- CloudWatch Logs Insights
   fields @timestamp, @duration, detail.event_type
   | filter @duration > 480000
   | sort @duration desc
   | limit 20
   ```

3. **Investigate root cause:**
   - DSL parsing slow? â†’ Profile with `cProfile`
   - API calls slow? â†’ Check Alpaca API latency
   - Large dataset? â†’ Optimize data processing

4. **Temporary mitigation:**
   - Increase Lambda timeout (max 15 min)
   - Add reserved concurrency
   - Scale down event batch size

---

## Future Improvements

### Short-term (Next Sprint)
- [ ] Add payload size CloudWatch metric (custom metric)
- [ ] Create CloudWatch dashboard for EventBridge health
- [ ] Add Slack integration for SNS topic
- [ ] Document S3 fallback pattern for large events

### Medium-term (Next Quarter)
- [ ] Implement automatic S3 fallback for large payloads
- [ ] Add event replay UI/CLI tool
- [ ] Profile Lambda cold starts (optimize if > 3s)
- [ ] Add distributed tracing (X-Ray)

### Long-term (Next Year)
- [ ] Migrate to Step Functions for complex workflows
- [ ] Implement event sourcing for full audit trail
- [ ] Add event schema registry (EventBridge Schema Registry)
- [ ] Performance testing under load (1000+ events/min)

---

## References

- [EventBridge Limits](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-quota.html)
- [CloudWatch Alarms Best Practices](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Best_Practice_Recommended_Alarms_AWS_Services.html)
- [Lambda Performance Optimization](https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html)
- [EventBridge Gotchas Document](./EVENTBRIDGE_GOTCHAS.md)

---

## Conclusion

All 3 high-priority fixes have been implemented and tested:

1. âœ… **Payload size validation** - Prevents silent failures
2. âœ… **DLQ monitoring** - Comprehensive alarms already in place
3. âœ… **Lambda timeout monitoring** - Early warning system added

**Status:** Ready for deployment ðŸš€

**Deployment Command:**
```bash
make deploy  # or: sam deploy --guided
```

**Post-Deployment:**
1. Subscribe to SNS topic for alerts
2. Monitor first production run closely
3. Validate alarms fire correctly (test in dev first)
