"""
Load Testing for Trading System

Tests system performance under realistic trading load conditions.
Simulates market open scenarios, high-volume trading periods, and sustained operations.
"""

import queue
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from decimal import Decimal

import numpy as np
import pandas as pd


@dataclass
class LoadTestMetrics:
    """Metrics collected during load testing."""

    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    avg_response_time: float = 0.0
    max_response_time: float = 0.0
    min_response_time: float = float("inf")
    throughput_per_second: float = 0.0
    error_rate: float = 0.0


class TradingSystemLoadTester:
    """Load testing framework for trading system components."""

    def __init__(self):
        self.metrics = LoadTestMetrics()
        self.response_times = []
        self._lock = threading.Lock()

    def record_request(self, response_time: float, success: bool):
        """Record metrics for a single request."""
        with self._lock:
            self.metrics.total_requests += 1

            if success:
                self.metrics.successful_requests += 1
            else:
                self.metrics.failed_requests += 1

            self.response_times.append(response_time)
            self.metrics.max_response_time = max(self.metrics.max_response_time, response_time)
            self.metrics.min_response_time = min(self.metrics.min_response_time, response_time)

    def calculate_final_metrics(self, test_duration: float):
        """Calculate final metrics after test completion."""
        if self.response_times:
            self.metrics.avg_response_time = np.mean(self.response_times)

        self.metrics.throughput_per_second = self.metrics.total_requests / test_duration

        if self.metrics.total_requests > 0:
            self.metrics.error_rate = self.metrics.failed_requests / self.metrics.total_requests

    def get_percentile_response_time(self, percentile: float) -> float:
        """Get response time at specific percentile."""
        if not self.response_times:
            return 0.0
        return np.percentile(self.response_times, percentile)


class TestMarketDataLoad:
    """Test load handling for market data processing."""

    def test_market_open_surge(self):
        """Test handling of market open data surge."""

        class MarketDataProcessor:
            def __init__(self):
                self.processed_count = 0
                self.errors = 0
                self.data_queue = queue.Queue(maxsize=2000)  # Increased queue size
                self.processing = True

            def add_market_data(self, data_point: dict) -> bool:
                """Add market data point for processing."""
                try:
                    self.data_queue.put(data_point, timeout=0.001)  # Very short timeout
                    return True
                except queue.Full:
                    return False

            def process_data_point(self, data_point: dict) -> bool:
                """Process a single market data point."""
                try:
                    # Simulate validation and processing
                    if data_point.get("price", 0) <= 0:
                        raise ValueError("Invalid price")

                    # Reduced processing time for better throughput
                    time.sleep(0.0001)  # 0.1ms processing time (reduced from 1ms)

                    self.processed_count += 1
                    return True

                except Exception:
                    self.errors += 1
                    return False

            def start_processing(self):
                """Start processing data from queue."""
                while self.processing:
                    try:
                        data_point = self.data_queue.get(timeout=0.05)  # Shorter timeout
                        self.process_data_point(data_point)
                        self.data_queue.task_done()
                    except queue.Empty:
                        continue

            def stop_processing(self):
                """Stop processing."""
                self.processing = False

        processor = MarketDataProcessor()

        # Start processing thread
        processing_thread = threading.Thread(target=processor.start_processing)
        processing_thread.start()

        # Simulate market open surge
        symbols = [f"STOCK{i:03d}" for i in range(100)]  # 100 symbols (reduced for faster test)
        surge_duration = 5  # seconds (reduced from 30 to avoid timeout)
        target_rate = 1000  # data points per second (reduced for stability)

        load_tester = TradingSystemLoadTester()
        start_time = time.time()

        # Generate market open surge
        total_data_points = 0
        successful_adds = 0

        while time.time() - start_time < surge_duration:
            batch_start = time.time()

            # Generate batch of data points
            batch_size = 50  # Reduced batch size for better queue management
            for _ in range(batch_size):
                symbol = np.random.choice(symbols)
                data_point = {
                    "symbol": symbol,
                    "price": np.random.uniform(10, 1000),
                    "volume": np.random.randint(1000, 100000),
                    "timestamp": time.time(),
                }

                request_start = time.time()
                success = processor.add_market_data(data_point)
                response_time = time.time() - request_start

                load_tester.record_request(response_time, success)
                total_data_points += 1

                if success:
                    successful_adds += 1

            # Rate limiting
            batch_duration = time.time() - batch_start
            expected_duration = batch_size / target_rate
            if batch_duration < expected_duration:
                time.sleep(expected_duration - batch_duration)

        # Stop processing and collect metrics
        test_duration = time.time() - start_time
        processor.stop_processing()

        # Wait for processing thread to finish with a reasonable timeout
        processing_thread.join(timeout=2)

        # Force thread cleanup if it's still alive
        if processing_thread.is_alive():
            processor.processing = False  # Ensure flag is set
            processing_thread.join(timeout=1)  # Give it one more second

        load_tester.calculate_final_metrics(test_duration)

        # Validate performance under surge conditions
        assert (
            load_tester.metrics.throughput_per_second > target_rate * 0.8
        ), f"Throughput too low: {load_tester.metrics.throughput_per_second:.0f}/sec"

        assert (
            load_tester.metrics.error_rate < 0.1
        ), f"Error rate too high: {load_tester.metrics.error_rate:.2%}"

        assert (
            load_tester.get_percentile_response_time(95) < 0.01
        ), f"95th percentile response time too high: {load_tester.get_percentile_response_time(95):.3f}s"

        # Check data processing
        assert (
            processor.processed_count > successful_adds * 0.7
        ), f"Too much data lost in processing pipeline: {processor.processed_count} processed vs {successful_adds} added"

    def test_sustained_trading_load(self):
        """Test sustained load during active trading hours."""

        class TradingEngine:
            def __init__(self):
                self.orders_placed = 0
                self.orders_filled = 0
                self.portfolio_updates = 0
                self.position_calculations = 0
                self.errors = 0
                self._lock = threading.Lock()

            def place_order(self, symbol: str, quantity: int, price: Decimal) -> dict:
                """Simulate order placement."""
                start_time = time.time()

                try:
                    # Simulate order validation
                    if quantity <= 0 or price <= 0:
                        raise ValueError("Invalid order parameters")

                    # Simulate broker API call
                    time.sleep(np.random.uniform(0.001, 0.005))  # 1-5ms API latency

                    with self._lock:
                        self.orders_placed += 1

                        # 95% fill rate
                        if np.random.random() < 0.95:
                            self.orders_filled += 1
                            status = "filled"
                        else:
                            status = "rejected"

                    return {
                        "status": status,
                        "symbol": symbol,
                        "quantity": quantity,
                        "price": price,
                        "processing_time": time.time() - start_time,
                    }

                except Exception:
                    with self._lock:
                        self.errors += 1
                    return {"status": "error", "processing_time": time.time() - start_time}

            def update_portfolio(self, orders: list) -> bool:
                """Update portfolio based on filled orders."""
                try:
                    # Simulate portfolio calculation
                    time.sleep(0.002)  # 2ms calculation time

                    with self._lock:
                        self.portfolio_updates += 1
                        self.position_calculations += len(orders)

                    return True

                except Exception:
                    with self._lock:
                        self.errors += 1
                    return False

        trading_engine = TradingEngine()
        load_tester = TradingSystemLoadTester()

        # Simulate sustained trading load
        symbols = ["AAPL", "GOOGL", "MSFT", "AMZN", "TSLA", "META", "NFLX", "NVDA"]
        test_duration = 10  # 10 seconds (reduced from 60 for faster tests)
        target_orders_per_second = 50

        def trading_worker(worker_id: int, duration: float):
            """Worker function for placing orders."""
            worker_start = time.time()
            worker_orders = 0

            while time.time() - worker_start < duration:
                symbol = np.random.choice(symbols)
                quantity = np.random.randint(1, 1000)
                price = Decimal(str(round(np.random.uniform(50, 500), 2)))

                request_start = time.time()
                result = trading_engine.place_order(symbol, quantity, price)
                response_time = time.time() - request_start

                success = result.get("status") in ["filled", "rejected"]  # Both are valid responses
                load_tester.record_request(response_time, success)
                worker_orders += 1

                # Rate limiting per worker
                time.sleep(0.02)  # 50 orders/second target

            return worker_orders

        # Run concurrent trading simulation
        num_workers = 4
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [
                executor.submit(trading_worker, i, test_duration) for i in range(num_workers)
            ]

            total_orders = sum(future.result() for future in as_completed(futures))

        actual_duration = time.time() - start_time
        load_tester.calculate_final_metrics(actual_duration)

        # Validate sustained performance
        assert (
            load_tester.metrics.throughput_per_second > target_orders_per_second * 0.8
        ), f"Order throughput too low: {load_tester.metrics.throughput_per_second:.0f}/sec"

        assert (
            load_tester.metrics.error_rate < 0.05
        ), f"Error rate too high: {load_tester.metrics.error_rate:.2%}"

        assert (
            load_tester.get_percentile_response_time(99) < 0.1
        ), f"99th percentile response time too high: {load_tester.get_percentile_response_time(99):.3f}s"

        # Check trading engine performance
        assert trading_engine.orders_filled > total_orders * 0.9, "Order fill rate too low"

        assert trading_engine.errors < total_orders * 0.02, "Too many trading engine errors"

    def test_portfolio_calculation_load(self):
        """Test portfolio calculation performance under load."""

        class PortfolioCalculator:
            def __init__(self):
                self.calculations_completed = 0
                self.calculation_errors = 0
                self._lock = threading.Lock()

            def calculate_portfolio_metrics(self, positions: dict, market_prices: dict) -> dict:
                """Calculate comprehensive portfolio metrics."""
                start_time = time.time()

                try:
                    total_value = Decimal("0")
                    total_cost_basis = Decimal("0")
                    position_values = {}

                    # Calculate position values
                    for symbol, position in positions.items():
                        if symbol in market_prices:
                            quantity = position["quantity"]
                            avg_price = position["avg_price"]
                            current_price = market_prices[symbol]

                            market_value = quantity * current_price
                            cost_basis = quantity * avg_price

                            position_values[symbol] = {
                                "market_value": market_value,
                                "cost_basis": cost_basis,
                                "unrealized_pnl": market_value - cost_basis,
                                "weight": 0,  # Will be calculated after total
                            }

                            total_value += market_value
                            total_cost_basis += cost_basis

                    # Calculate weights
                    for symbol in position_values:
                        if total_value > 0:
                            position_values[symbol]["weight"] = float(
                                position_values[symbol]["market_value"] / total_value
                            )

                    # Simulate additional calculations
                    time.sleep(0.005)  # 5ms calculation time

                    result = {
                        "total_value": total_value,
                        "total_cost_basis": total_cost_basis,
                        "total_unrealized_pnl": total_value - total_cost_basis,
                        "position_count": len(position_values),
                        "positions": position_values,
                        "calculation_time": time.time() - start_time,
                    }

                    with self._lock:
                        self.calculations_completed += 1

                    return result

                except Exception:
                    with self._lock:
                        self.calculation_errors += 1
                    return {
                        "error": "calculation_failed",
                        "calculation_time": time.time() - start_time,
                    }

        calculator = PortfolioCalculator()
        load_tester = TradingSystemLoadTester()

        # Generate large portfolio
        num_positions = 200
        symbols = [f"STOCK{i:04d}" for i in range(num_positions)]

        portfolio_positions = {}
        market_prices = {}

        for symbol in symbols:
            portfolio_positions[symbol] = {
                "quantity": np.random.randint(10, 1000),
                "avg_price": Decimal(str(round(np.random.uniform(10, 500), 2))),
            }
            market_prices[symbol] = Decimal(str(round(np.random.uniform(10, 500), 2)))

        # Test concurrent portfolio calculations
        test_duration = 5  # seconds (reduced from 30 for faster tests)
        target_calculations_per_second = 20

        def calculation_worker(worker_id: int, duration: float):
            """Worker for portfolio calculations."""
            worker_start = time.time()
            calculations = 0

            while time.time() - worker_start < duration:
                # Simulate price updates
                for symbol in np.random.choice(symbols, 10):  # Update 10 random prices
                    price_change = np.random.uniform(-0.05, 0.05)  # Â±5% change
                    market_prices[symbol] *= Decimal(str(1 + price_change))

                request_start = time.time()
                result = calculator.calculate_portfolio_metrics(portfolio_positions, market_prices)
                response_time = time.time() - request_start

                success = "error" not in result
                load_tester.record_request(response_time, success)
                calculations += 1

                # Rate limiting
                time.sleep(1.0 / target_calculations_per_second)

            return calculations

        # Run concurrent calculations
        num_workers = 3
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [
                executor.submit(calculation_worker, i, test_duration) for i in range(num_workers)
            ]

            total_calculations = sum(future.result() for future in as_completed(futures))

        actual_duration = time.time() - start_time
        load_tester.calculate_final_metrics(actual_duration)

        # Validate portfolio calculation performance
        assert (
            load_tester.metrics.throughput_per_second > target_calculations_per_second * 0.8
        ), f"Calculation throughput too low: {load_tester.metrics.throughput_per_second:.0f}/sec"

        assert (
            load_tester.metrics.error_rate < 0.01
        ), f"Calculation error rate too high: {load_tester.metrics.error_rate:.2%}"

        assert (
            load_tester.get_percentile_response_time(95) < 0.02
        ), f"95th percentile calculation time too high: {load_tester.get_percentile_response_time(95):.3f}s"

        # Check calculator performance
        assert (
            calculator.calculations_completed > total_calculations * 0.98
        ), "Too many failed calculations"


class TestScalabilityLimits:
    """Test system scalability limits and resource constraints."""

    def test_maximum_concurrent_operations(self):
        """Test maximum number of concurrent operations."""

        class ConcurrentOperationManager:
            def __init__(self, max_concurrent=100):
                self.max_concurrent = max_concurrent
                self.active_operations = 0
                self.completed_operations = 0
                self.rejected_operations = 0
                self._lock = threading.Lock()
                self._semaphore = threading.Semaphore(max_concurrent)

            def execute_operation(self, operation_id: int) -> dict:
                """Execute an operation with concurrency control."""
                acquired = self._semaphore.acquire(blocking=False)

                if not acquired:
                    with self._lock:
                        self.rejected_operations += 1
                    return {"status": "rejected", "reason": "max_concurrency_reached"}

                try:
                    with self._lock:
                        self.active_operations += 1

                    # Simulate operation
                    operation_time = np.random.uniform(0.1, 0.5)
                    time.sleep(operation_time)

                    with self._lock:
                        self.completed_operations += 1

                    return {
                        "status": "completed",
                        "operation_id": operation_id,
                        "duration": operation_time,
                    }

                finally:
                    with self._lock:
                        self.active_operations -= 1
                    self._semaphore.release()

        manager = ConcurrentOperationManager(max_concurrent=50)

        # Launch more operations than the limit
        num_operations = 200
        operation_results = []

        def launch_operation(op_id: int):
            return manager.execute_operation(op_id)

        start_time = time.time()

        with ThreadPoolExecutor(max_workers=num_operations) as executor:
            futures = [executor.submit(launch_operation, i) for i in range(num_operations)]
            operation_results = [future.result() for future in as_completed(futures)]

        duration = time.time() - start_time

        # Analyze results
        completed = sum(1 for r in operation_results if r["status"] == "completed")
        rejected = sum(1 for r in operation_results if r["status"] == "rejected")

        # Should have completed some operations
        assert completed > 0, "No operations completed"

        # Should have rejected operations beyond capacity
        assert rejected > 0, "No operations rejected despite exceeding capacity"

        # Should not exceed maximum concurrent operations
        assert manager.completed_operations == completed
        assert manager.rejected_operations == rejected

        # Performance should be reasonable
        throughput = completed / duration
        assert throughput > 20, f"Throughput too low: {throughput:.1f} ops/sec"

    def test_memory_scaling_with_data_volume(self):
        """Test memory usage scaling with increasing data volume."""
        import gc
        import os

        import psutil

        process = psutil.Process(os.getpid())

        def process_large_dataset(num_symbols: int, days: int) -> dict:
            """Process market data for multiple symbols."""
            # Generate large market dataset
            data_points = []

            for symbol_idx in range(num_symbols):
                symbol = f"STOCK{symbol_idx:04d}"
                base_price = np.random.uniform(50, 500)

                for _day in range(days):
                    # Multiple intraday points
                    for _minute in range(390):  # 6.5 hour trading day
                        price_change = np.random.normal(0, 0.001)
                        base_price *= 1 + price_change

                        data_points.append(
                            {
                                "symbol": symbol,
                                "timestamp": "2024-01-01 09:30:00",  # Simplified timestamp
                                "price": round(base_price, 2),
                                "volume": np.random.randint(100, 10000),
                            }
                        )

            # Convert to DataFrame and perform operations
            df = pd.DataFrame(data_points)

            # Perform memory-intensive operations
            df["price_change"] = df.groupby("symbol")["price"].pct_change()
            df["volume_ma"] = (
                df.groupby("symbol")["volume"].rolling(20).mean().reset_index(0, drop=True)
            )
            df["price_ma"] = (
                df.groupby("symbol")["price"].rolling(50).mean().reset_index(0, drop=True)
            )

            # Calculate summary statistics
            summary = (
                df.groupby("symbol")
                .agg(
                    {
                        "price": ["mean", "std", "min", "max"],
                        "volume": ["mean", "sum"],
                        "price_change": ["mean", "std"],
                    }
                )
                .reset_index()
            )

            return {
                "total_data_points": len(df),
                "unique_symbols": df["symbol"].nunique(),
                "summary_stats": len(summary),
            }

        # Test with increasing data volumes
        test_cases = [
            {"symbols": 10, "days": 5},
            {"symbols": 25, "days": 10},
            {"symbols": 50, "days": 15},
            {"symbols": 100, "days": 20},
        ]

        memory_measurements = []

        for test_case in test_cases:
            # Force garbage collection before test
            gc.collect()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB

            start_time = time.time()
            result = process_large_dataset(test_case["symbols"], test_case["days"])
            processing_time = time.time() - start_time

            # Force garbage collection after test
            gc.collect()
            end_memory = process.memory_info().rss / 1024 / 1024  # MB

            memory_used = end_memory - start_memory
            data_points = result["total_data_points"]

            memory_measurements.append(
                {
                    "symbols": test_case["symbols"],
                    "days": test_case["days"],
                    "data_points": data_points,
                    "memory_used_mb": memory_used,
                    "processing_time": processing_time,
                    "memory_per_point_kb": (
                        (memory_used * 1024) / data_points if data_points > 0 else 0
                    ),
                    "throughput_points_per_sec": (
                        data_points / processing_time if processing_time > 0 else 0
                    ),
                }
            )

        # Validate memory scaling
        for measurement in memory_measurements:
            # Memory per data point should be reasonable
            assert (
                measurement["memory_per_point_kb"] < 5.0
            ), f"Memory usage too high: {measurement['memory_per_point_kb']:.2f} KB/point"

            # Processing should be reasonable
            assert (
                measurement["throughput_points_per_sec"] > 1000
            ), f"Processing too slow: {measurement['throughput_points_per_sec']:.0f} points/sec"

        # Memory efficiency shouldn't degrade too much with scale
        if len(memory_measurements) >= 2:
            small_efficiency = memory_measurements[0]["memory_per_point_kb"]
            large_efficiency = memory_measurements[-1]["memory_per_point_kb"]

            # Large datasets shouldn't be more than 3x less memory efficient
            efficiency_ratio = large_efficiency / small_efficiency if small_efficiency > 0 else 1
            assert (
                efficiency_ratio < 3.0
            ), f"Memory efficiency degrades too much: {efficiency_ratio:.2f}x"
